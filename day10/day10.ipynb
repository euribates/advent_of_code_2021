{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10: Syntax Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            yield line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(load_data('10-sample.txt'))\n",
    "assert lines[0] == '[({(<(())[]>[[{[]{<()<>>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_SYMBOLS = \"([{<\"\n",
    "CLOSE_SYMBOLS = \")]}>\"\n",
    "\n",
    "def match(open_symbol, close_symbol):\n",
    "    assert open_symbol in OPEN_SYMBOLS\n",
    "    assert close_symbol in CLOSE_SYMBOLS\n",
    "    return OPEN_SYMBOLS.index(open_symbol) == CLOSE_SYMBOLS.index(close_symbol)\n",
    "\n",
    "assert match('(', ')') is True\n",
    "assert match('[', ']') is True\n",
    "assert match('{', '}') is True\n",
    "assert match('<', '>') is True\n",
    "assert match('(', ']') is False\n",
    "assert match('(', '}') is False\n",
    "assert match('(', '>') is False\n",
    "assert match('[', '>') is False\n",
    "with pytest.raises(AssertionError):\n",
    "    match('(', 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_for(c):\n",
    "    assert c in OPEN_SYMBOLS\n",
    "    index = OPEN_SYMBOLS.index(c)\n",
    "    return CLOSE_SYMBOLS[index]\n",
    "\n",
    "assert expected_for('(') == ')'\n",
    "assert expected_for('[') == ']'\n",
    "assert expected_for('{') == '}'\n",
    "assert expected_for('<') == '>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Result:\n",
    "    expected: str\n",
    "    found: str\n",
    "        \n",
    "    def __bool__(self):\n",
    "        return self.expected == self.found\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.expected != self.found:\n",
    "            return f\"Expected {self.expected}, but found {self.found} instead.\"\n",
    "        return \"\"\n",
    "    \n",
    "    def is_corrupted(self):\n",
    "        return self.found != ''\n",
    "    \n",
    "    \n",
    "r = Result(expected=\"}\", found=\">\")\n",
    "assert bool(r) is False\n",
    "assert str(r) == 'Expected }, but found > instead.'\n",
    "\n",
    "r = Result(expected=\"}\", found=\"}\")\n",
    "assert bool(r) is True\n",
    "assert str(r) == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line(line):\n",
    "    stack = []\n",
    "    for c in line:\n",
    "        if c in OPEN_SYMBOLS:\n",
    "            stack.append(c)\n",
    "        elif c in CLOSE_SYMBOLS:\n",
    "            top = stack.pop()\n",
    "            if not match(top, c):\n",
    "                return Result(expected=expected_for(top), found=c)\n",
    "    if stack:\n",
    "        return Result(expected=[expected_for(c) for c in stack], found='')\n",
    "    return Result(expected='', found='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `()` is a legal chunk that contains no other chunks, as is `[]`. More complex but valid chunks include `([])`, `{()()()}`, `<([{}])>`, `[<>({}){}[([])<>]]`, and even `(((((((((())))))))))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert check_line(\"()\")\n",
    "assert check_line(\"([])\")\n",
    "assert check_line(\"{()()()}\")\n",
    "assert check_line(\"<([{}])>\")\n",
    "assert check_line(\"[<>({}){}[([])<>]]\")\n",
    "assert check_line(\"(((((((((())))))))))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corrupted line is one where a chunk closes with the wrong character - that is, where the characters it opens and closes with do not form one of the four legal pairs listed above.\n",
    "\n",
    "Examples of corrupted chunks include `(]`, `{()()()>`, `(((()))}`, and `<([]){()}[{}])`. Such a chunk can appear anywhere within a line, and its presence causes the whole line to be considered corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not check_line(\"(]\")\n",
    "assert not check_line(\"{()()()>\")\n",
    "assert not check_line(\"(((()))}\")\n",
    "assert not check_line(\"<([]){()}[{}])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the lines aren't corrupted, just incomplete; you can ignore these lines for now. The remaining five lines are corrupted:\n",
    "\n",
    "```\n",
    "{([(<{}[<>[]}>{[]{[(<()> - Expected ], but found } instead.\n",
    "[[<[([]))<([[{}[[()]]] - Expected ], but found ) instead.\n",
    "[{[{({}]{}}([{[{{{}}([] - Expected ), but found ] instead.\n",
    "[<(<(<(<{}))><([]([]() - Expected >, but found ) instead.\n",
    "<{([([[(<>()){}]>(<<{{ - Expected ], but found > instead.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{([(<{}[<>[]}>{[]{[(<()> Expected ], but found } instead. }\n",
      "[[<[([]))<([[{}[[()]]] Expected ], but found ) instead. )\n",
      "[{[{({}]{}}([{[{{{}}([] Expected ), but found ] instead. ]\n",
      "[<(<(<(<{}))><([]([]() Expected >, but found ) instead. )\n",
      "<{([([[(<>()){}]>(<<{{ Expected ], but found > instead. >\n"
     ]
    }
   ],
   "source": [
    "for line in load_data('10-sample.txt'):\n",
    "    chk = check_line(line)\n",
    "    if not chk and chk.is_corrupted():\n",
    "        print(line, chk, chk.found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you know that syntax checkers actually have contests to see who can get the high score for syntax errors in a file? It's true! To calculate the syntax error score for a line, take the **first illegal character on the line** and look it up in the following table:\n",
    "\n",
    "| char | points        |\n",
    "|------|---------------|\n",
    "| `)`  | 3 points      |\n",
    "| `]`  | 57 points     |\n",
    "| `}`  | 1197 points   |\n",
    "| `>`  | 25137 points  |\n",
    "\n",
    "In the above example, an illegal `)` was found twice (2*3 = 6 points), an illegal `]` was found once (57 points), an illegal `}` was found once (1197 points), and an illegal `>` was found once (25137 points). So, the total syntax error score for this file is:\n",
    "\n",
    "$$ 6+57+1197+25137 = 26397 $$\n",
    "\n",
    "**26397** points!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(lines):\n",
    "    points = {\n",
    "        \")\": 3,\n",
    "        \"]\": 57,\n",
    "        \"}\": 1197,\n",
    "        \">\": 25137,\n",
    "    }\n",
    "    result = 0\n",
    "    for line in lines:\n",
    "        chk = check_line(line)\n",
    "        if not chk and chk.is_corrupted():\n",
    "            result += points[chk.found]\n",
    "    return result\n",
    "            \n",
    "assert score(load_data('10-sample.txt')) ==  26397           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution for part one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution of part one is: 318099\n"
     ]
    }
   ],
   "source": [
    "sol = score(load_data('10-input.txt')) \n",
    "print(f\"Solution of part one is: {sol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can only use closing characters (), ], }, or >), and you must add them in the correct order so that only legal pairs are formed and all chunks end up closed.\n",
    "\n",
    "In the example above, there are five incomplete lines:\n",
    "\n",
    "- `[({(<(())[]>[[{[]{<()<>>` - Complete by adding `}}]])})]`.\n",
    "- `[(()[<>])]({[<{<<[]>>(` - Complete by adding `)}>]})`.\n",
    "- `(((({<>}<{<{<>}{[]{[]{}` - Complete by adding `}}>}>))))`.\n",
    "- `{<[[]]>}<{[{[{[]{()[[[]` - Complete by adding `]]}}]}]}>`.\n",
    "- `<{([{{}}[<[[[<>{}]]]>[]]` - Complete by adding `])}>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({(<(())[]>[[{[]{<()<>> False }}]])})]\n",
      "[(()[<>])]({[<{<<[]>>( False )}>]})\n",
      "(((({<>}<{<{<>}{[]{[]{} False }}>}>))))\n",
      "{<[[]]>}<{[{[{[]{()[[[] False ]]}}]}]}>\n",
      "<{([{{}}[<[[[<>{}]]]>[]] False ])}>\n"
     ]
    }
   ],
   "source": [
    "for line in load_data('10-sample.txt'):\n",
    "    chk = check_line(line)\n",
    "    if not chk.is_corrupted():\n",
    "        print(line, bool(chk), ''.join(reversed(chk.expected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you know that autocomplete tools also have contests? It's true! The score is determined by considering the completion string character-by-character. Start with a total score of 0. Then, for each character, multiply the total score by 5 and then increase the total score by the point value given for the character in the following table:\n",
    "\n",
    "| symbol | points    |\n",
    "|--------|-----------|\n",
    "| `)`    | 1 point   |\n",
    "| `]`    | 2 points  |\n",
    "| `}`    | 3 points  |\n",
    "| `>`    | 4 points  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete_score(chars, tron=False):\n",
    "    points = {\n",
    "        \")\": 1,\n",
    "        \"]\": 2,\n",
    "        \"}\": 3,\n",
    "        \">\": 4,\n",
    "    }\n",
    "    acc = 0\n",
    "    for c in chars:\n",
    "        if tron: print(acc, end=\" -> \")\n",
    "        acc *= 5\n",
    "        acc += points[c]\n",
    "        if tron: print(acc)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the last completion string above - `])}>` - would be scored as follows:\n",
    "\n",
    "- Start with a total score of $0$.\n",
    "- Multiply the total score by $5$ to get $0$, then add the value of ] (2) to get a new total score of $2$.\n",
    "- Multiply the total score by $5$ to get $10$, then add the value of ) (1) to get a new total score of $11$.\n",
    "- Multiply the total score by $5$ to get $55$, then add the value of } (3) to get a new total score of $58$.\n",
    "- Multiply the total score by $5$ to get $290$, then add the value of > (4) to get a new total score of $294$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 2\n",
      "2 -> 11\n",
      "11 -> 58\n",
      "58 -> 294\n"
     ]
    }
   ],
   "source": [
    "assert autocomplete_score('])}>', tron=True) == 294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five lines' completion strings have total scores as follows:\n",
    "\n",
    "- `}}]])})]` - 288957 total points.\n",
    "- `)}>]})` - 5566 total points.\n",
    "- `}}>}>))))` - 1480781 total points.\n",
    "- `]]}}]}]}>` - 995444 total points.\n",
    "- `])}>` - 294 total points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({(<(())[]>[[{[]{<()<>> False 288957\n",
      "[(()[<>])]({[<{<<[]>>( False 5566\n",
      "(((({<>}<{<{<>}{[]{[]{} False 1480781\n",
      "{<[[]]>}<{[{[{[]{()[[[] False 995444\n",
      "<{([{{}}[<[[[<>{}]]]>[]] False 294\n"
     ]
    }
   ],
   "source": [
    "for line in load_data('10-sample.txt'):\n",
    "    chk = check_line(line)\n",
    "    if not chk.is_corrupted():\n",
    "        print(line, bool(chk), autocomplete_score(reversed(chk.expected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution part two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocomplete tools are an odd bunch: the winner is found by sorting all of the scores and then taking the middle score. (There will always be an odd number of scores to consider.) In this example, the middle score is $288957$ because there are the same number of scores smaller and larger than it.\n",
    "\n",
    "Find the completion string for each incomplete line, score the completion strings, and sort the scores. What is the middle score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for line in load_data('10-sample.txt'):\n",
    "    chk = check_line(line)\n",
    "    if not chk.is_corrupted():\n",
    "        score = autocomplete_score(reversed(chk.expected))\n",
    "        items.append(score)\n",
    "items.sort()  \n",
    "sol = items[len(items)//2]\n",
    "assert sol == 288957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution part two: 2389738699\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "for line in load_data('10-input.txt'):\n",
    "    chk = check_line(line)\n",
    "    if not chk.is_corrupted():\n",
    "        score = autocomplete_score(reversed(chk.expected))\n",
    "        items.append(score)\n",
    "items.sort()  \n",
    "sol = items[len(items)//2]\n",
    "print(f\"Solution part two: {sol}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
